# LightRFT

<div align="center">

<img src="assets/logo.png" alt="LightRFT Logo" width="600"/>

**Light, Efficient, Omni-modal & Reward-model Driven Reinforcement Fine-Tuning Framework**

[![Version](https://img.shields.io/badge/version-0.1.0-blue.svg)](https://github.com/DeepLink-org/lightrft)
[![Python](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1+-ee4c2c.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-Apache%202.0-green.svg)](LICENSE)

English | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh.md)

</div>

---

## üì¢ Project Overview

LightRFT is a lightweight post-training framework for LLM models, designed to support post-training exploration in the security and multimodal domains.

## üìñ Introduction

**LightRFT** (Light Reinforcement Fine-Tuning) is an advanced reinforcement learning fine-tuning framework designed for Large Language Models (LLMs) and Vision-Language Models (VLMs). This framework provides efficient and scalable RLHF (Reinforcement Learning from Human Feedback) and RLVR training capabilities, supporting multiple state-of-the-art algorithms and distributed training strategies.

### ‚ú® Key Features

- üöÄ **High-Performance Inference Engines**
  - Integrated vLLM and SGLang for efficient sampling and inference
  - FP8 inference optimization for significantly reduced latency and memory usage
  - Flexible engine sleep/wake mechanisms for optimal resource utilization

- üß† **Rich Algorithm Ecosystem** 
  - **Policy Optimization**: GRPO, GSPO, GMPO, Dr.GRPO
  - **Advantage Estimation**: REINFORCE++, CPGD
  - **Reward Processing**: Reward Norm/Clip
  - **Sampling Strategy**: FIRE Sampling, Token-Level Policy
  - **Stability Enhancement**: DAPO, select_high_entropy_tokens

- üîß **Flexible Training Strategies**
  - FSDP (Fully Sharded Data Parallel) v2 support
  - DeepSpeed ZeRO (Stage 1/2/3) support
  - Gradient checkpointing and mixed precision training (BF16/FP16)
  - Adam Offload and memory optimization techniques
  - Support packing samples

- üéØ **Innovative Resource Collaboration**
  - **Colocate Anything**: Co-locate reward models with training models to maximize GPU utilization
    - Support multiple reward models for parallel inference on the same device
    - Dynamic memory management with automatic training/inference phase switching
    - Reduced cross-device communication overhead for improved end-to-end training efficiency
  - **Balance Anything** üöß (Under Development): Intelligent load balancing system
    - Adaptive task scheduling and resource allocation
    - Automatic load balancing for multi-node training
    - Performance optimization for heterogeneous hardware environments

- üåê **Comprehensive Multimodal Support**
  - **Native Vision-Language Model (VLM) Training**
    - Support for mainstream VLMs like Qwen-VL, InternVL
    - Parallel processing of multimodal image-text data
    - Support sequence parallel
    - Efficient multimodal tokenization and batching
    - Support processing one text with more than one images
  - **Multimodal Reward Modeling**
    - Support for multiple visual reward models working in collaboration
    - Joint optimization of image understanding and text generation
    - Support reward model as a service
  - **Complete Vision-Language Alignment Training Pipeline**
    - Optimized for multimodal RLVR/RLHF training
    - Built-in support for vision-language model fine-tuning

- üìä **Complete Experimental Toolkit**
  - Weights & Biases (W&B) integration
  - Math capability benchmarking (GSM8K, Geo3K, etc.)
  - Trajectory saving and analysis tools
  - Automatic checkpoint management

- ‚úàÔ∏è **Efficient Performance Optimization Strategies**
  - Support data load balancing during mixed text-image training, reducing training time by 30%
  - Support dynamic batch size
  - Support memory optimization for logprobs calculation

---

## üéØ Supported Algorithms

For detailed algorithm descriptions, implementation details, and usage guide, see [Algorithm Documentation](docs/source/quick_start/algorithms.md).

| Algorithm | Type | Key Improvement | Paper |
|-----------|------|-----------------|-------|
| **GRPO** | Policy Optimization | Group normalized advantage estimation |  [arXiv:2402.03300](https://arxiv.org/pdf/2402.03300)  |
| **GSPO** | Policy Optimization | Group sequence policy optimization | [arXiv:2507.18071](https://arxiv.org/abs/2507.18071) |
| **GMPO (WIP)** | Policy Optimization | Geometric-mean policy optimization | [arXiv:2507.20673](https://arxiv.org/abs/2507.20673) |
| **Dr.GRPO** | Policy Optimization | Length bias mitigation | [arXiv:2503.20783](https://arxiv.org/abs/2503.20783) |
| **DAPO** | Policy Optimization | Decoupled clip and dynamic sampling policy optimization | [arXiv:2503.14476](https://arxiv.org/abs/2503.14476) |
| **REINFORCE++** | Advantage Estimation | Improved baseline estimation | [arXiv:2501.03262](https://arxiv.org/abs/2501.03262) |
| **CPGD** | Advantage Estimation | KL-based drift constraint | [arXiv:2505.12504](https://arxiv.org/abs/2505.12504) |
| **FIRE Sampling** | Sampling Strategy | Filtering and ranking strategies | [arXiv:2410.21236](https://arxiv.org/abs/2410.21236) |

---

## üöÄ Quick Start

### Requirements

- Python >= 3.10
- CUDA >= 12.8
- PyTorch >= 2.5.1

### Docker Images

TO BE DONE

### Installation

Clone and install LightRFT:

```bash
# Clone the repository
git clone https://github.com/DeepLink-org/LightRFT.git
cd LightRFT

# Install dependencies
pip install -r requirements.txt

# Install LightRFT
pip install -e .
```


## üìö Usage Guide

### Basic Example: GRPO Training

```bash
# Single node, 8 GPU training example
cd LightRFT

# Run GRPO training (GSM8K math reasoning task)
bash examples/gsm8k_geo3k/run_grpo_gsm8k_qwen2.5_0.5b.sh

# Or run Geo3K geometry problem training (VLM multimodal)
bash examples/gsm8k_geo3k/run_grpo_geo3k_qwen2.5_vl_7b.sh
```

---

## üèóÔ∏è Project Structure

```
LightRFT/
‚îú‚îÄ‚îÄ lightrft/                      # Core library
‚îÇ   ‚îú‚îÄ‚îÄ strategy/                  # Training & inference strategies
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fsdp/                  # FSDP implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deepspeed/             # DeepSpeed implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vllm_utils/            # vLLM utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sglang_utils/          # SGLang utilities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/                 # Strategy utilities
‚îÇ   ‚îú‚îÄ‚îÄ models/                    # Model definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ actor_al.py            # Audio-language model actor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ actor_language.py      # Language model actor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ actor_vl.py            # Vision-language model actor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grm_vl.py              # Generative reward model (Vision-Language)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ srm_al.py              # Scalar reward model (Audio-Language)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ srm_vl.py              # Scalar reward model (Vision-Language)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loss.py                # Loss functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ monkey_patch/          # Model adaptation patches for distributed training
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tests/                 # Model tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py               # Model utilities
‚îÇ   ‚îú‚îÄ‚îÄ trainer/                   # Trainer implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ppo_trainer.py         # LLM PPO trainer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ppo_trainer_vl.py      # VLM PPO trainer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spmd_ppo_trainer.py    # SPMD PPO trainer Extension (Core)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grm_trainer_vl.py      # Generative reward model trainer (Vision-Language)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ srm_trainer_al.py      # Scalar reward model trainer (Audio-Language)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ srm_trainer_vl.py      # Scalar reward model trainer (Vision-Language)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fast_exp_maker.py      # Fast experience generator (Core)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ experience_maker.py    # Base experience generator
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ experience_maker_vl.py # Base experience generator for VLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ replay_buffer.py       # Replay buffer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ replay_buffer_vl.py    # VLM replay buffer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ replay_buffer_utils.py # Replay buffer utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kl_controller.py       # KL divergence controller
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py               # Trainer utilities
‚îÇ   ‚îú‚îÄ‚îÄ datasets/                  # Dataset processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ audio_alpaca.py        # Audio Alpaca dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grm_dataset.py         # Generative reward model dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hpdv3.py               # HPDv3 reward model dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image_reward_db.py     # Image reward database
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ imagegen_cot_reward.py # Image generation CoT generative reward
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ omnirewardbench.py     # OmniRewardBench dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ process_reward_dataset.py # Reward dataset processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts_dataset.py     # LLM Prompts dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts_dataset_vl.py  # Vision-language prompts dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rapidata.py            # Rapidata reward modeldataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sft_dataset.py         # SFT dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sft_dataset_vl.py      # VLM SFT dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ srm_dataset.py         # Scalar reward model base dataset
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py               # Dataset utilities
‚îÇ   ‚îî‚îÄ‚îÄ utils/                     # Utility functions
‚îÇ       ‚îú‚îÄ‚îÄ ckpt_scripts/          # Checkpoint processing scripts
‚îÇ       ‚îú‚îÄ‚îÄ cli_args.py            # CLI argument parsing
‚îÇ       ‚îú‚îÄ‚îÄ distributed_sampler.py # Distributed sampler
‚îÇ       ‚îú‚îÄ‚îÄ logging_utils.py       # Logging utilities
‚îÇ       ‚îú‚îÄ‚îÄ processor.py           # Data processor for HF model
‚îÇ       ‚îú‚îÄ‚îÄ remote_rm_utils.py     # Remote reward model utilities
‚îÇ       ‚îú‚îÄ‚îÄ timer.py               # Timer utilities
‚îÇ       ‚îú‚îÄ‚îÄ trajectory_saver.py    # Trajectory saver
‚îÇ       ‚îî‚îÄ‚îÄ utils.py               # General utilities
‚îÇ
‚îú‚îÄ‚îÄ examples/                      # Usage examples
‚îÇ   ‚îú‚îÄ‚îÄ gsm8k_geo3k/               # GSM8K/Geo3K math reasoning training examples
‚îÇ   ‚îú‚îÄ‚îÄ grm_training/              # Generative reward model training examples
‚îÇ   ‚îú‚îÄ‚îÄ srm_training/              # Scalar reward model training examples
‚îÇ   ‚îú‚îÄ‚îÄ chat/                      # Model dialogue examples
‚îÇ
‚îú‚îÄ‚îÄ docs/                          # üìö Sphinx documentation
‚îÇ   ‚îú‚îÄ‚îÄ Makefile                   # Documentation build Makefile
‚îÇ   ‚îú‚îÄ‚îÄ make.bat                   # Documentation build batch file
‚îÇ   ‚îî‚îÄ‚îÄ source/                    # Documentation source
‚îÇ       ‚îú‚îÄ‚îÄ _static/               # Static files (CSS, etc.)
‚îÇ       ‚îú‚îÄ‚îÄ api_doc/               # API documentation
‚îÇ       ‚îú‚îÄ‚îÄ best_practice/         # Best practices & resources
‚îÇ       ‚îú‚îÄ‚îÄ installation/          # Installation guides
‚îÇ       ‚îî‚îÄ‚îÄ quick_start/           # Quick start & user guides
‚îÇ
‚îú‚îÄ‚îÄ assets/                        # Assets
‚îÇ   ‚îî‚îÄ‚îÄ logo.png                   # Project logo
‚îÇ
‚îú‚îÄ‚îÄ CHANGELOG.md                   # Changelog
‚îú‚îÄ‚îÄ LICENSE                        # License file
‚îú‚îÄ‚îÄ Makefile                       # Project Makefile
‚îú‚îÄ‚îÄ README.md                      # Project documentation (English)
‚îú‚îÄ‚îÄ README_zh.md                   # Project documentation (Chinese)
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îú‚îÄ‚îÄ requirements-dev.txt           # Development dependencies
‚îú‚îÄ‚îÄ requirements-doc.txt           # Documentation dependencies
‚îî‚îÄ‚îÄ setup.py                       # Package setup script
```

### üîë Key Directory Descriptions

- **`lightrft/`**: LightRFT core library, providing training strategies, model definitions, and trainer implementations
- **`examples/`**: Complete training examples and scripts
  - `gsm8k_geo3k/`: GSM8K and Geo3K math reasoning training examples
  - `grm_training/`: Generative reward model training examples
  - `srm_training/`: Scalar reward model training examples
  - `chat/`: Model dialogue examples
- **`docs/`**: Sphinx documentation with complete user guides and API documentation

---

## ‚öôÔ∏è Key Configuration Parameters

### Batch Size Configuration

```bash
TBS=128                           # Training batch size
RBS=128                            # Rollout batch size
micro_train_batch_size=1          # Micro batch size per GPU
micro_rollout_batch_size=2        # Rollout micro batch size
```

### Algorithm Parameters

```bash
--advantage_estimator group_norm  # Advantage estimator: group_norm, reinforce, cpgd
--n_samples_per_prompt 8          # Number of samples per prompt
--max_epochs 1                    # Training epochs per episode
--num_episodes 3                  # Total training episodes
--kl_estimator k3                 # KL estimator type
--init_kl_coef 0.001              # KL penalty coefficient
```

### Distributed Training

```bash
--fsdp                            # Enable FSDP
--zero_stage 3                    # DeepSpeed ZeRO Stage
--gradient_checkpointing          # Gradient checkpointing
--adam_offload                    # Adam optimizer offload
--bf16                            # BF16 mixed precision
```

### Inference Engine

```bash
--rm_use_engine                   # Use inference engine (vLLM/SGLang) for reward model
--engine_mem_util 0.4             # Engine memory utilization
--engine_tp_size 1                # Engine tensor parallelism degree
--enable_engine_sleep             # Enable engine sleep mechanism
```

---

## ‚ö° Performance Optimization Recommendations

### Memory Optimization

1. Use gradient checkpoint

```bash
--gradient_checkpointing
```

2. Use Adam Offload

```bash
--adam_offload
```

3. adjust engine memory utilization

```bash
--engine_mem_util 0.4
```

4. environment variable optimization
```bash
export TORCH_NCCL_AVOID_RECORD_STREAMS=1
```

### Calculation optimization

1. FP8 rollout (only in the inference stage)
  - Reduce inference latency and VRAM usage, while maintaining BF16 precision during training.

```bash
--fp8_rollout
--enable_vllm_is_correction
```

2. Flash Attention

```bash
--flash_attn
```

3. batch size optimize

Recommend: train_batch_size >= rollout_batch_size √ó n_samples_per_prompt

---

## üîß Troubleshooting

See training scripts for detailed parameter validation logic.

### 1. OOM (Out of Memory)

**Solutions**:
- Reduce `micro_train_batch_size` and `micro_rollout_batch_size`
- Enable `--gradient_checkpointing`
- Lower `--engine_mem_util`
- Use ZeRO Stage 3

### 2. Training Instability

**Solutions**:
- Enable Reward Normalization: `--normalize_reward`
- Lower learning rate
- Use `--advantage_estimator group_norm`
- Try DAPO algorithm

---

## üìñ Documentation

### üìö Complete Documentation Guide

**Quick Start:**
- [Installation Guide](docs/source/installation/index.rst) - Docker images, installation methods, and troubleshooting
- [Supported Algorithms](docs/source/quick_start/algorithms.md) - Comprehensive algorithm guide with implementation details
- [Configuration Reference](docs/source/quick_start/configuration.md) - Complete parameter documentation

**Best Practices:**
- [Training Strategy Usage](docs/source/best_practice/strategy.rst) - FSDP, DeepSpeed, and inference engine configuration
- [FAQ](docs/source/best_practice/faq.md) - Frequently asked questions and solutions
- [Troubleshooting Guide](docs/source/best_practice/troubleshooting.md) - Common issues and debugging
- [Contributing Guide](docs/source/best_practice/contributing.md) - How to contribute to LightRFT

### Build Documentation Locally

Install documentation dependencies:
```bash
pip install -r requirements-doc.txt
```

Generate HTML documentation:
```bash
make docs
# Open docs/build/index.html to view documentation
```

Live documentation preview:
```bash
make docs-live
# Visit http://localhost:8000
```

## ü§ù Contributing

We welcome community contributions! Please follow these steps:

1. Fork this repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Code Standards

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Code formatting (YAPF)
make format

# Code linting (Flake8)
make fcheck
```

---

## üìö Citation

If you use this codebase in your research or applications, please cite it as follows:

```bibtex
@misc{lightrft,
  title={LightRFT},
  author={Niu, Yazhe and Pu, Yuan and Shi, Dongxing and Lu, Yudong and Xiong, Yingtong and Ge, Ruijun and Sun, Jiaxuan and Wan, Zunian and Zhang, Shaoang and others},
  publisher={GitHub},
  howpublished={\url{https://github.com/DeepLink-org/LightRFT}},
  year={2025},
}
```

## ‚å®Ô∏è Development Team

- Business Team Framework Group: Responsible for the development of the algorithm ecosystem, training strategies, multimodal support, and the experimental toolchain, focusing on algorithm innovation and the enhancement of model training capabilities.
- System Team DeepLink: Responsible for high-performance inference engines, resource coordination mechanisms, system-level performance optimization, and underlying infrastructure, focusing on the optimization of system performance and resource utilization efficiency.

---

## üìÑ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

**LightRFT**‚Äã is collaboratively developed by **The RL Team‚Äã of the Safe and Trustworthy Center** and **The System Platform Center DeepLink Team**‚Äã at Shanghai AI Laboratory. We extend our sincere thanks to the contributors from both teams.
- The reinforcement learning component of this project is developed based on [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF), and we express our heartfelt gratitude to its development team for their outstanding work.
- We thank the Qwen, InternVL, and DeepSeek teams for providing excellent open-source foundation models.
- We also acknowledge the powerful tools provided by open-source communities such as DeepSpeed, PyTorch, vLLM, and SGLang.

### Open Source Dependencies

This project builds upon the following outstanding open-source projects (including but not limited):

- **[OpenRLHF](https://github.com/OpenRLHF/OpenRLHF)**, **[verl](https://github.com/volcengine/verl)** - Core RL framework foundation (parts of key components adapted and reused)
- [vLLM](https://github.com/vllm-project/vllm) - High-performance inference engine
- [SGLang](https://github.com/sgl-project/sglang) - Structured generation language runtime
- [DeepSpeed](https://github.com/microsoft/DeepSpeed) - Distributed training optimization
- [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html) - Fully Sharded Data Parallel

Thanks to all contributors and supporters!

---

## üóìÔ∏è RoadMap

We are actively working on the following improvements and features:

### Core Feature Enhancements

- [ ] **Trajectory Functionality Extension**
  - Add more analysis metrics
  - Enhanced trajectory saving and analysis capabilities

- [ ] **Reward Mechanism Refactoring**
  - Refactor rule-based and model-based reward computation
  - Optimize reward dataset processing pipeline

### Algorithm Optimization & Integration

- [ ] **More Algorithm Integration**
  - Entropy-based token selection 
  - GMPO (Geometric-Mean Policy Optimization)
  - GSPO (Group Sequence Policy Optimization)

- [ ] **Advantage Computation Refactoring**
  - Optimize advantage estimation module architecture
  - Unify advantage computation interface across algorithms

- [ ] **Loss-Filter Mechanism Optimization**
  - Refactor loss filtering implementation
  - Complete GSM8K/Geo3K benchmark experiments
  - Document experimental results and analysis



Community contributions and feedback are welcome!

---

## üìÆ Contact

For questions or suggestions, please contact us via:

- **Issues**: [GitHub Issues](https://github.com/DeepLink-org/LightRFT/issues)
- **Email**: DeepLink-org@pjlab.org.cn


---

<div align="center">

**‚≠ê If this project helps you, please give us a star!**

Made with ‚ù§Ô∏è by LightRFT Team

</div>
